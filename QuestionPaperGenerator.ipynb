{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import os\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import asyncio\n",
        "from dotenv import load_dotenv\n",
        "import nest_asyncio\n",
        "from groq import Groq\n",
        "import pdfplumber\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import io\n",
        "import fitz  # PyMuPDF\n",
        "import tempfile\n",
        "import re\n",
        "import time\n",
        "import shutil\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Initialize Sentence Transformer for embeddings\n",
        "embedding_model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
        "\n",
        "# Initialize FAISS index\n",
        "dimension = 384\n",
        "faiss_index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "# Memory to store previously generated questions\n",
        "question_memory = set()\n",
        "\n",
        "# Store chunk to document mapping\n",
        "chunk_doc_mapping = []\n",
        "\n",
        "# Initialize Groq GPT-4 client\n",
        "api_key = (\"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\")\n",
        "if not api_key:\n",
        "    raise ValueError(\"GROQ_API_KEY is missing. Set it in the .env file.\")\n",
        "client = Groq(api_key=api_key)\n",
        "\n",
        "# Function to extract text from PDFs with OCR capability\n",
        "def extract_text_from_pdfs(pdf_files):\n",
        "    start_time = time.time()\n",
        "    all_texts = []  # Store text with document ID\n",
        "    processed_files = 0\n",
        "\n",
        "    for doc_id, pdf_file in enumerate(pdf_files):\n",
        "        try:\n",
        "            # Get the file path directly - Gradio provides the file path\n",
        "            pdf_path = pdf_file.name\n",
        "\n",
        "            # First try with pdfplumber for text-based PDFs\n",
        "            text_from_plumber = \"\"\n",
        "            with pdfplumber.open(pdf_path) as pdf:\n",
        "                for page in pdf.pages:\n",
        "                    extracted = page.extract_text()\n",
        "                    if extracted:\n",
        "                        text_from_plumber += extracted + \" \"\n",
        "\n",
        "            # If pdfplumber extracted substantial text, use it\n",
        "            if len(text_from_plumber.strip()) > 100:  # Arbitrary threshold\n",
        "                all_texts.append((text_from_plumber, doc_id, os.path.basename(pdf_path)))\n",
        "            else:\n",
        "                # Fall back to OCR with PyMuPDF\n",
        "                doc = fitz.open(pdf_path)\n",
        "                ocr_text = \"\"\n",
        "                for page_num in range(len(doc)):\n",
        "                    page = doc.load_page(page_num)\n",
        "                    pix = page.get_pixmap(matrix=fitz.Matrix(300/72, 300/72))\n",
        "                    img = Image.open(io.BytesIO(pix.tobytes(\"png\")))\n",
        "                    ocr_text += pytesseract.image_to_string(img) + \"\\n\"\n",
        "                doc.close()\n",
        "                all_texts.append((ocr_text, doc_id, os.path.basename(pdf_path)))\n",
        "\n",
        "            processed_files += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error extracting text from {os.path.basename(pdf_file.name)}: {str(e)}\"\n",
        "\n",
        "    end_time = time.time()\n",
        "    processing_time = end_time - start_time\n",
        "\n",
        "    return all_texts, processed_files, processing_time\n",
        "\n",
        "# Split text into chunks with document tracking\n",
        "def split_text_into_chunks(text, doc_id, doc_name, chunk_size=750):\n",
        "    global chunk_doc_mapping\n",
        "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
        "    chunks, chunk = [], \"\"\n",
        "    chunk_ids = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if len(chunk) + len(sentence) <= chunk_size:\n",
        "            chunk += \" \" + sentence\n",
        "        else:\n",
        "            chunks.append(chunk.strip())\n",
        "            chunk_ids.append((doc_id, doc_name))\n",
        "            chunk = sentence\n",
        "\n",
        "    if chunk:\n",
        "        chunks.append(chunk.strip())\n",
        "        chunk_ids.append((doc_id, doc_name))\n",
        "\n",
        "    # Update global mapping\n",
        "    chunk_doc_mapping.extend(chunk_ids)\n",
        "    return chunks\n",
        "\n",
        "# Store embeddings in FAISS asynchronously with batching\n",
        "async def store_embeddings_in_faiss(text_chunks):\n",
        "    batch_size = 100\n",
        "    for i in range(0, len(text_chunks), batch_size):\n",
        "        batch = text_chunks[i:i + batch_size]\n",
        "        embeddings = embedding_model.encode(batch)\n",
        "        faiss_index.add(np.array(embeddings).astype('float32'))\n",
        "        # Small pause to prevent resource overload\n",
        "        if i + batch_size < len(text_chunks):\n",
        "            await asyncio.sleep(0.1)\n",
        "\n",
        "# Retrieve relevant text chunks using FAISS with improved diversity\n",
        "def retrieve_relevant_text(query, num_docs, min_chunks_per_doc=2, max_total_chunks=20):\n",
        "    # Calculate a reasonable number of chunks to retrieve\n",
        "    base_chunks = min_chunks_per_doc * num_docs\n",
        "    total_chunks = min(base_chunks + 5, max_total_chunks)  # Add some extras for relevance\n",
        "\n",
        "    # Get more chunks than we need to ensure document coverage\n",
        "    query_embedding = embedding_model.encode([query])\n",
        "    distances, indices = faiss_index.search(np.array(query_embedding).astype('float32'), total_chunks * 2)\n",
        "    retrieved_indices = indices[0]\n",
        "\n",
        "    # Ensure document diversity in the selection\n",
        "    selected_indices = select_diverse_chunks(retrieved_indices, num_docs, min_chunks_per_doc)\n",
        "\n",
        "    return selected_indices[:total_chunks]  # Limit to reasonable number\n",
        "\n",
        "# Select diverse chunks to ensure all documents are represented\n",
        "def select_diverse_chunks(retrieved_indices, num_docs, min_per_doc=1):\n",
        "    global chunk_doc_mapping\n",
        "\n",
        "    # Initialize tracking\n",
        "    selected_chunks = []\n",
        "    doc_counts = {i: 0 for i in range(num_docs)}\n",
        "    doc_chunks = {i: [] for i in range(num_docs)}\n",
        "\n",
        "    # First sort each chunk into its document bucket\n",
        "    for idx in retrieved_indices:\n",
        "        if idx < len(chunk_doc_mapping):\n",
        "            doc_id = chunk_doc_mapping[idx][0]\n",
        "            if doc_id < num_docs:  # Ensure valid doc_id\n",
        "                doc_chunks[doc_id].append(idx)\n",
        "\n",
        "    # First ensure minimum representation from each document\n",
        "    for doc_id in range(num_docs):\n",
        "        chunks = doc_chunks.get(doc_id, [])\n",
        "        for i, chunk_idx in enumerate(chunks):\n",
        "            if i < min_per_doc:  # Take at least min_per_doc from each document\n",
        "                selected_chunks.append(chunk_idx)\n",
        "                doc_counts[doc_id] += 1\n",
        "\n",
        "    # Then add remaining chunks in order of relevance (they were already sorted by distance)\n",
        "    for idx in retrieved_indices:\n",
        "        if idx < len(chunk_doc_mapping) and idx not in selected_chunks:\n",
        "            selected_chunks.append(idx)\n",
        "\n",
        "    return selected_chunks\n",
        "\n",
        "# Evaluate question quality\n",
        "def evaluate_question(question):\n",
        "    if question in question_memory:\n",
        "        return False\n",
        "    question_memory.add(question)\n",
        "    return True\n",
        "\n",
        "# Function to calculate mark allocation based on difficulty and question type\n",
        "def calculate_mark_allocation(difficulty, num_mcq, num_subjective, total_marks=100):\n",
        "    # Adjust base mark values according to difficulty\n",
        "    if difficulty == \"Easy\":\n",
        "        mcq_base_marks = 1\n",
        "        subjective_base_marks_min = 5\n",
        "        subjective_base_marks_max = 10\n",
        "    elif difficulty == \"Medium\":\n",
        "        mcq_base_marks = 2\n",
        "        subjective_base_marks_min = 8\n",
        "        subjective_base_marks_max = 15\n",
        "    else:  # Hard\n",
        "        mcq_base_marks = 3\n",
        "        subjective_base_marks_min = 10\n",
        "        subjective_base_marks_max = 20\n",
        "\n",
        "    # Calculate how many marks to allocate to MCQs and subjective questions\n",
        "    total_mcq_marks = mcq_base_marks * num_mcq\n",
        "\n",
        "    # Remaining marks go to subjective questions with variability\n",
        "    remaining_marks = total_marks - total_mcq_marks\n",
        "\n",
        "    # If there are subjective questions, distribute marks among them\n",
        "    if num_subjective > 0:\n",
        "        # Create a distribution of marks for subjective questions\n",
        "        subjective_marks = []\n",
        "\n",
        "        # For complex distribution, assign variable marks based on complexity\n",
        "        # First, ensure minimum marks for each question\n",
        "        min_marks_per_subjective = subjective_base_marks_min\n",
        "        total_min_subjective = min_marks_per_subjective * num_subjective\n",
        "\n",
        "        # If minimum exceeds remaining, adjust proportionally\n",
        "        if total_min_subjective > remaining_marks:\n",
        "            min_marks_per_subjective = remaining_marks // num_subjective\n",
        "            subjective_marks = [min_marks_per_subjective] * num_subjective\n",
        "        else:\n",
        "            # Distribute remaining marks with some questions worth more\n",
        "            extra_marks = remaining_marks - total_min_subjective\n",
        "            subjective_marks = [min_marks_per_subjective] * num_subjective\n",
        "\n",
        "            # Distribute extra marks to create variability\n",
        "            import random\n",
        "            # Set a seed for reproducibility\n",
        "            random.seed(42)\n",
        "\n",
        "            # Distribute extra marks with bias toward later questions\n",
        "            weights = [i+1 for i in range(num_subjective)]  # Higher weight for later questions\n",
        "            total_weight = sum(weights)\n",
        "\n",
        "            for i in range(num_subjective):\n",
        "                # Calculate proportional share of extra marks\n",
        "                extra_for_this = int((weights[i] / total_weight) * extra_marks)\n",
        "                subjective_marks[i] += extra_for_this\n",
        "\n",
        "            # Assign any remaining marks to the last question\n",
        "            remaining = remaining_marks - sum(subjective_marks)\n",
        "            if remaining > 0:\n",
        "                subjective_marks[-1] += remaining\n",
        "\n",
        "        # Ensure no question exceeds maximum marks for difficulty\n",
        "        for i in range(len(subjective_marks)):\n",
        "            if subjective_marks[i] > subjective_base_marks_max:\n",
        "                excess = subjective_marks[i] - subjective_base_marks_max\n",
        "                subjective_marks[i] = subjective_base_marks_max\n",
        "\n",
        "                # Redistribute excess marks to other questions below max\n",
        "                for j in range(len(subjective_marks)):\n",
        "                    if j != i and subjective_marks[j] < subjective_base_marks_max and excess > 0:\n",
        "                        room = subjective_base_marks_max - subjective_marks[j]\n",
        "                        add = min(room, excess)\n",
        "                        subjective_marks[j] += add\n",
        "                        excess -= add\n",
        "                        if excess == 0:\n",
        "                            break\n",
        "    else:\n",
        "        subjective_marks = []\n",
        "\n",
        "    return mcq_base_marks, subjective_marks\n",
        "\n",
        "# Generate questions using Groq\n",
        "def generate_questions(relevant_chunks, text_chunks, subject, num_questions, num_mcq, num_subjective, difficulty, doc_count, total_marks=100, question_types=None):\n",
        "    questions = []\n",
        "\n",
        "    # Get mark allocation\n",
        "    mcq_marks, subjective_marks = calculate_mark_allocation(difficulty, num_mcq, num_subjective, total_marks)\n",
        "\n",
        "    # Get the corresponding text chunks and document info\n",
        "    doc_representation = {}\n",
        "    relevant_text = \"\"\n",
        "\n",
        "    for idx in relevant_chunks:\n",
        "        if idx < len(text_chunks) and idx < len(chunk_doc_mapping):\n",
        "            chunk_text = text_chunks[idx]\n",
        "            doc_id, doc_name = chunk_doc_mapping[idx]\n",
        "\n",
        "            # Track which documents are represented\n",
        "            if doc_id not in doc_representation:\n",
        "                doc_representation[doc_id] = 1\n",
        "            else:\n",
        "                doc_representation[doc_id] += 1\n",
        "\n",
        "            # Add document source to the text\n",
        "            relevant_text += f\"\\n--- From document: {doc_name} ---\\n{chunk_text}\\n\"\n",
        "\n",
        "    # Define question types if not provided\n",
        "    if not question_types:\n",
        "        question_types = {\n",
        "            \"bloom\": True,\n",
        "            \"competency\": True,\n",
        "            \"case_study\": False,\n",
        "            \"fill_blanks\": False\n",
        "        }\n",
        "\n",
        "    # Format the mark information for the prompt\n",
        "    mcq_mark_info = f\"Each MCQ question is worth {mcq_marks} marks.\"\n",
        "\n",
        "    subjective_mark_info = \"\"\n",
        "    if subjective_marks:\n",
        "        subjective_mark_info = \"Subjective questions have the following mark allocation:\\n\"\n",
        "        for i, marks in enumerate(subjective_marks):\n",
        "            subjective_mark_info += f\"- Question {i+1}: {marks} marks\\n\"\n",
        "\n",
        "    # Build a prompt that emphasizes using ALL documents with taxonomies but removes visible tags\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert in {subject}. Generate {num_questions} unique and high-quality questions with marks assigned to each question.\n",
        "\n",
        "    QUESTION STRUCTURE:\n",
        "    - {num_mcq} questions should be in \"Section - 1 MCQ\". These should be multiple-choice (MCQ) questions with 4 options each.\n",
        "    - {num_subjective} questions should be in \"Section - 2 Subjective\". These should be subjective/essay questions.\n",
        "    - Ensure the difficulty level is {difficulty}.\n",
        "\n",
        "    MARK ALLOCATION:\n",
        "    The total exam is worth {total_marks} marks.\n",
        "    {mcq_mark_info}\n",
        "    {subjective_mark_info}\n",
        "\n",
        "    FORMATTING REQUIREMENTS:\n",
        "    - Include the mark value in parentheses after each question number, like: \"Question 1 ({mcq_marks} marks)\"\n",
        "    - For subjective questions, show the specific mark value like: \"Question 1 ({subjective_marks[0] if subjective_marks else 0} marks)\"\n",
        "\n",
        "    - At the start of the question paper, include a line showing \"Total Marks: {total_marks}\"\n",
        "\n",
        "    IMPORTANT METADATA REQUIREMENTS:\n",
        "    \"\"\"\n",
        "\n",
        "    if question_types.get(\"bloom\", False):\n",
        "        prompt += \"\"\"\n",
        "    BLOOM'S TAXONOMY (INTERNAL USE ONLY):\n",
        "    Internally categorize questions across different cognitive levels of Bloom's Taxonomy:\n",
        "    1. Remember (Knowledge): Questions testing recall of facts, terms, basic concepts\n",
        "    2. Understand (Comprehension): Questions testing understanding of meaning\n",
        "    3. Apply: Questions testing use of information in new situations\n",
        "    4. Analyze: Questions testing breaking down of information and exploring relationships\n",
        "    5. Evaluate: Questions testing justification of decisions or actions\n",
        "    6. Create: Questions testing generation of new ideas or products\n",
        "\n",
        "    IMPORTANT: DO NOT add any visible Bloom's Taxonomy labels or tags to the questions in your final output.\n",
        "    \"\"\"\n",
        "\n",
        "    if question_types.get(\"competency\", False):\n",
        "        prompt += \"\"\"\n",
        "    COMPETENCY-BASED CATEGORIES (INTERNAL USE ONLY):\n",
        "    Internally categorize questions that assess different competencies:\n",
        "    - Conceptual Understanding: Tests grasp of fundamental concepts\n",
        "    - Critical Thinking: Tests ability to evaluate information and make judgments\n",
        "    - Problem-Solving: Tests ability to apply knowledge to solve real-world problems\n",
        "    - Communication: Tests ability to articulate ideas effectively\n",
        "    - Research Skills: Tests ability to gather and interpret information\n",
        "\n",
        "    IMPORTANT: DO NOT add any visible competency category labels or tags to the questions in your final output.\n",
        "    \"\"\"\n",
        "\n",
        "    if question_types.get(\"case_study\", False):\n",
        "        prompt += \"\"\"\n",
        "    CASE STUDY:\n",
        "    Include at least one case study question that presents a scenario and asks multiple related questions about it.\n",
        "    DO NOT explicitly label it as a case study in the output.\n",
        "    \"\"\"\n",
        "\n",
        "    if question_types.get(\"fill_blanks\", False):\n",
        "        prompt += \"\"\"\n",
        "    FILL IN THE BLANKS:\n",
        "    Include fill-in-the-blank questions where appropriate.\n",
        "    Format them clearly with underscores or blank spaces to indicate where answers should be filled in.\n",
        "    DO NOT explicitly label them as fill-in-the-blank questions.\n",
        "    \"\"\"\n",
        "\n",
        "    prompt += f\"\"\"\n",
        "    IMPORTANT OUTPUT REQUIREMENTS:\n",
        "    - DO NOT include any taxonomy labels, tags, or categorization markers in the final questions.\n",
        "    - The questions should appear clean without any [Bloom level], {{competency}}, or other classification tags.\n",
        "    - Create questions that cover content from ALL {doc_count} documents provided.\n",
        "    - Try to distribute questions evenly across all the source documents.\n",
        "    - Ensure questions are non-repetitive and cover different concepts.\n",
        "    - Provide a mix of question formats while maintaining the MCQ and subjective counts.\n",
        "    - In the final section titled \"ANSWER KEY\", provide the answers for all questions and for each answer, specify which document it's based on (e.g., [Source: document1.pdf]).\n",
        "    - At the very end, include a \"MARK DISTRIBUTION\" section that shows the total marks and breakdown by section.\n",
        "\n",
        "    Here's the reference text with document sources:\n",
        "    {relevant_text}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"llama-3.3-70b-versatile\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=2500  # Increased to allow for more comprehensive responses\n",
        "        )\n",
        "        generated_text = response.choices[0].message.content.strip()\n",
        "\n",
        "        # Post-process to remove any taxonomy tags that might have been included despite instructions\n",
        "        processed_text = re.sub(r'\\[(Remember|Understand|Apply|Analyze|Evaluate|Create)\\]', '', generated_text)\n",
        "        processed_text = re.sub(r'\\{(Conceptual Understanding|Critical Thinking|Problem-Solving|Communication|Research Skills)\\}', '', processed_text)\n",
        "        processed_text = re.sub(r'\\[CASE STUDY\\]', '', processed_text)\n",
        "        processed_text = re.sub(r'\\[FILL IN THE BLANKS\\]', '', processed_text)\n",
        "\n",
        "        # Clean up any double spaces resulting from tag removal\n",
        "        processed_text = re.sub(r' +', ' ', processed_text)\n",
        "\n",
        "        questions = processed_text.split(\"\\n\\n\")  # Splitting based on paragraphs\n",
        "\n",
        "        # Ensure uniqueness\n",
        "        questions = [q for q in questions if evaluate_question(q)]\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error generating questions: {str(e)}\"\n",
        "\n",
        "    return \"\\n\\n\".join(questions)\n",
        "\n",
        "# Function to add missing marks from the original implementation\n",
        "def generate_question_bank(pdf_files, subject, num_questions, num_mcq, num_subjective, difficulty, question_types, total_marks=100):\n",
        "    global faiss_index, chunk_doc_mapping\n",
        "\n",
        "    # Reset global variables for new processing\n",
        "    faiss_index = faiss.IndexFlatL2(dimension)\n",
        "    chunk_doc_mapping = []\n",
        "    question_memory.clear()\n",
        "\n",
        "    try:\n",
        "        # Extract text from PDFs\n",
        "        result = extract_text_from_pdfs(pdf_files)\n",
        "        if isinstance(result, str):  # Error occurred\n",
        "            return result\n",
        "\n",
        "        all_texts, processed_files, processing_time = result\n",
        "\n",
        "        # Process the texts into chunks\n",
        "        all_chunks = []\n",
        "        for text, doc_id, doc_name in all_texts:\n",
        "            chunks = split_text_into_chunks(text, doc_id, doc_name)\n",
        "            all_chunks.extend(chunks)\n",
        "\n",
        "        # Store embeddings asynchronously\n",
        "        asyncio.run(store_embeddings_in_faiss(all_chunks))\n",
        "\n",
        "        # Create a query based on subject and difficulty\n",
        "        query = f\"{subject} {difficulty} level questions\"\n",
        "\n",
        "        # Retrieve relevant chunks\n",
        "        relevant_indices = retrieve_relevant_text(query, len(all_texts))\n",
        "\n",
        "        # Generate questions with mark allocation\n",
        "        questions = generate_questions(relevant_indices, all_chunks, subject,\n",
        "                                     num_questions, num_mcq, num_subjective,\n",
        "                                     difficulty, len(all_texts), total_marks, question_types)\n",
        "\n",
        "        return questions\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error in question bank generation: {str(e)}\"\n",
        "\n",
        "# Process function with metadata for internal tracking but clean output\n",
        "def process_with_status(pdf_files, subject, num_questions, num_mcq, num_subjective, difficulty,\n",
        "                       bloom_taxonomy, competency_based, case_study, fill_blanks, total_marks=100):\n",
        "    status_value = f\"â³ Processing {len(pdf_files)} PDFs...\"\n",
        "    yield status_value, \"\"\n",
        "\n",
        "    question_types = {\n",
        "        \"bloom\": bloom_taxonomy,\n",
        "        \"competency\": competency_based,\n",
        "        \"case_study\": case_study,\n",
        "        \"fill_blanks\": fill_blanks\n",
        "    }\n",
        "\n",
        "    result = generate_question_bank(pdf_files, subject, num_questions, num_mcq, num_subjective,\n",
        "                                   difficulty, question_types, total_marks)\n",
        "\n",
        "    # Additional cleanup to ensure no tags are visible\n",
        "    clean_result = re.sub(r'\\[(Remember|Understand|Apply|Analyze|Evaluate|Create)\\]\\s*', '', result)\n",
        "    clean_result = re.sub(r'\\{(Conceptual Understanding|Critical Thinking|Problem-Solving|Communication|Research Skills)\\}\\s*', '', clean_result)\n",
        "    clean_result = re.sub(r'\\[CASE STUDY\\]\\s*', '', clean_result)\n",
        "    clean_result = re.sub(r'\\[FILL IN THE BLANKS\\]\\s*', '', clean_result)\n",
        "    clean_result = re.sub(r' +', ' ', clean_result)\n",
        "\n",
        "    status_value = \"âœ… Enhanced question bank generation complete!\"\n",
        "    yield status_value, clean_result\n",
        "\n",
        "# Enhanced Gradio UI with a modern, aesthetic design including total marks\n",
        "with gr.Blocks(css=\"\"\"\n",
        "    body { background: linear-gradient(to right, #4A00E0, #8E2DE2); color: white; }\n",
        "    .gradio-container { background: #ffffff; border-radius: 12px; box-shadow: 0 4px 15px rgba(0, 0, 0, 0.2); padding: 25px; }\n",
        "    .gradio-button { background: linear-gradient(45deg, #ff416c, #ff4b2b); color: white; font-weight: bold; border-radius: 8px; border: none; padding: 12px 20px; transition: 0.3s; }\n",
        "    .gradio-button:hover { transform: scale(1.05); }\n",
        "    .gr-textbox { border: 2px solid #6a11cb; border-radius: 8px; padding: 8px; }\n",
        "    .progress-text { color: #ff416c; font-weight: bold; }\n",
        "    h1 { text-align: center; font-size: 2rem; margin-bottom: 10px; }\n",
        "    h3 { color: #6a11cb; text-align: center; margin-bottom: 20px; }\n",
        "    .category-title { font-weight: bold; margin-top: 10px; color: #6a11cb; }\n",
        "    \"\"\") as demo:\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    # âœ¨ Enhanced Question Bank Generator\n",
        "    **AI-Powered with Advanced Learning Frameworks**\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        pdf_files = gr.Files(label=\"ðŸ“„ Upload PDFs\", file_types=[\".pdf\"])\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            subject = gr.Textbox(label=\"ðŸ“š Subject\", placeholder=\"Enter subject name\", elem_classes=[\"gr-textbox\"])\n",
        "            difficulty = gr.Dropdown(label=\"ðŸŽ¯ Difficulty Level\", choices=[\"Easy\", \"Medium\", \"Hard\"], value=\"Medium\")\n",
        "            total_marks = gr.Slider(label=\"ðŸ“Š Total Marks\", minimum=50, maximum=200, value=100, step=10)\n",
        "\n",
        "        with gr.Column():\n",
        "            num_questions = gr.Slider(label=\"â“ Total Questions\", minimum=1, maximum=20, value=10, step=1)\n",
        "            num_mcq = gr.Slider(label=\"ðŸ”˜ MCQs\", minimum=0, maximum=15, value=5, step=1)\n",
        "            num_subjective = gr.Slider(label=\"âœï¸ Subjective\", minimum=0, maximum=15, value=5, step=1)\n",
        "\n",
        "    with gr.Row():\n",
        "        gr.Markdown(\"\"\"### ðŸ§  Question Categories (Internal Classification Only)\"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            bloom_taxonomy = gr.Checkbox(label=\"Use Bloom's Taxonomy (internal only)\", value=True)\n",
        "            bloom_info = gr.Markdown(\"\"\"\n",
        "            <div class=\"category-title\">Questions will be created using these cognitive levels:</div>\n",
        "            - Remember: Knowledge recall\n",
        "            - Understand: Comprehension\n",
        "            - Apply: Use in new situations\n",
        "            - Analyze: Break down information\n",
        "            - Evaluate: Justify decisions\n",
        "            - Create: Generate new ideas\n",
        "            \"\"\")\n",
        "\n",
        "        with gr.Column():\n",
        "            competency_based = gr.Checkbox(label=\"Use Competency-Based Categories (internal only)\", value=True)\n",
        "            competency_info = gr.Markdown(\"\"\"\n",
        "            <div class=\"category-title\">Questions will be created using these competencies:</div>\n",
        "            - Conceptual Understanding\n",
        "            - Critical Thinking\n",
        "            - Problem-Solving\n",
        "            - Communication\n",
        "            - Research Skills\n",
        "            \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            case_study = gr.Checkbox(label=\"Include Case Study Questions\", value=False)\n",
        "        with gr.Column():\n",
        "            fill_blanks = gr.Checkbox(label=\"Include Fill-in-the-Blanks\", value=False)\n",
        "\n",
        "    status = gr.Textbox(label=\"ðŸš€ Status\", value=\"Ready to process\", interactive=False)\n",
        "    generate_button = gr.Button(\"âš¡ Generate Question Bank with Marks\", elem_classes=[\"gradio-button\"])\n",
        "    output = gr.Textbox(label=\"ðŸ“œ Generated Question Bank\", lines=25, interactive=False)\n",
        "\n",
        "    generate_button.click(\n",
        "        process_with_status,\n",
        "        inputs=[pdf_files, subject, num_questions, num_mcq, num_subjective, difficulty,\n",
        "               bloom_taxonomy, competency_based, case_study, fill_blanks, total_marks],\n",
        "        outputs=[status, output]\n",
        "    )\n",
        "\n",
        "# Add an exception handler to catch any errors during launch\n",
        "try:\n",
        "    demo.launch(share=True)\n",
        "except Exception as e:\n",
        "    print(f\"Error launching Gradio app: {str(e)}\")"
      ],
      "metadata": {
        "id": "FnjLIp0eMnye",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "outputId": "937f5c72-68e6-40df-9f23-c0c842383952"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://57c9440556f9ca008e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://57c9440556f9ca008e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error launching Gradio app: module 'gradio' has no attribute 'blocks'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZxF1zvDQMoaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AeQwS6Cw9k5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kYDe5m7h9k7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x7gF_w9L9k9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u2r2sPoR9k-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eK4Mm2tJ9yYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "46wkvliSRV2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vlzDB7RORV4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "BvBinIn_RV77",
        "outputId": "c6129d42-5342-44e7-9c10-650c288260c5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://c5b2bad6ff58fdffb6.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c5b2bad6ff58fdffb6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error launching Gradio app: module 'gradio' has no attribute 'blocks'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kFWQJvj0RWow"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}